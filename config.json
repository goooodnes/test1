{
  "address": "",
  "description": "Optimized Llama-3.2-3B Q3 for maximum speed, no quality constraints.",
  "domain": "gaia.domains",
  "llamaedge_port": "8080",
  "server_health_url": "https://pulse.gaianet.ai/node-health/0x",
  "server_info_url": "https://pulse.gaianet.ai/node-info/0x",
  "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q3_K_M.gguf",
  "chat_name": "llama-3.2-3b-q3",
  "chat_ctx_size": "2048",
  "chat_batch_size": "64",
  "chat_ubatch_size": "64",
  "prompt_template": "llama-3-chat",
  "reverse_prompt": "",
  "system_prompt": "You are a high-speed assistant. Respond instantly without overthinking.",
  "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
  "embedding_name": "nomic-embed",
  "embedding_ctx_size": "2048",
  "embedding_batch_size": "2048",
  "snapshot": "",
  "embedding_collection_name": "default",
  "qdrant_limit": "1",
  "qdrant_score_threshold": "0.1",
  "rag_policy": "default",
  "rag_prompt": "Respond as quickly as possible without considering accuracy.\n----------------\n"
}
